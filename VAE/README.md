## [ë…¼ë¬¸ì •ë¦¬]Auto-Encoding Variational Bayes (2013), Diederik P Kingma, Max Welling

### Problem statement
- ì—°ì†ì ì¸ ì ì¬ ë³€ìˆ˜ì™€ ê³„ì‚° ë¶ˆê°€ëŠ¥í•œ ì‚¬í›„ ë¶„í¬ê°€ ìˆëŠ” ê²½ìš°ì—ë„, ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ì„œ íš¨ìœ¨ì ìœ¼ë¡œ í™•ë¥  ëª¨ë¸ì—ì„œ ì¶”ë¡  ë° í•™ìŠµì„ ìˆ˜í–‰
    - ì—°ì†ì ì¸ ì ì¬ ë³€ìˆ˜ : ê° ì´ë¯¸ì§€ë§ˆë‹¤ í•´ë‹¹ ì´ë¯¸ì§€ì˜ íŠ¹ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” í•˜ë‚˜ì˜ ì ì¬ ë³€ìˆ˜ ì¡´ì¬í•œë‹¤ê³  ê°€ì •í•˜ë©´, ì ì¬ ë³€ìˆ˜ë“¤ì€ ì—°ì†ì ì¸ ê°’ìœ¼ë¡œ ë³€í•  ìˆ˜ ìˆìŒ
    - ê³„ì‚° ë¶ˆê°€ëŠ¥í•œ ì‚¬í›„ ë¶„í¬ : ë°ì´í„° xê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ì ì¬ ë³€ìˆ˜ zì˜ ë¶„í¬
- ë³€ë¶„ í•˜í•œì„ ìµœì í™”í•˜ê¸° ìœ„í•œ ê²½ì‚¬ ê³„ì‚°ì˜ ë¬¸ì œ ì œê¸°(ë†’ì€ ë¶„ì‚°ì„ ê°€ì§€ëŠ” ëª¬í…Œì¹´ë¥¼ë¡œ ê²½ì‚¬ ì¶”ì •ë°©ì‹ì„ ëŒ€ì²´í•  ë°©ë²• í•„ìš” (-> ì¶”í›„ì— SGVB ì œì•ˆ))
    - ë³€ë¶„ ì¶”ë¡ ì—ì„œëŠ”, p(x)ë¥¼ ì§ì ‘ ê³„ì‚°í•˜ì§€ ì•Šê³ , ë³€ë¶„ í•˜í•œì„ í†µí•´ ê·¼ì‚¬í•¨
    - ë³€ë¶„ í•˜í•œ (Variational Lower Bound, ELBO) : Marginal Likelihoodë¥¼ Lower Boundë¡œ ê·¼ì‚¬í•˜ëŠ” ê°’, VAEì˜ ì†ì‹¤ í•¨ìˆ˜     
          ![image](https://github.com/user-attachments/assets/54872add-1d95-42bf-a8a0-252e2013fff9)   
          (ìœ„ ê·¸ë¦¼ì—ì„œ Lì´ ë³€ë¶„í•˜í•œ)
          ![image](https://github.com/user-attachments/assets/5142a1ad-5ff6-4989-b9a2-969bc6f6284d)   
          (ë³€ë¶„ í•˜í•œì€, Marginal Likelihoodë¥¼ KL Divergenceë¡œ ë¶„í•´í•˜ì—¬ ìœ ë„ë¨. KL DivergenceëŠ” í•­ìƒ 0 ì´ìƒì´ë¯€ë¡œ, Lì´ í•˜í•œì´ ë¨)   
          ![image](https://github.com/user-attachments/assets/b17a6b1b-53f8-4fa6-9942-d3d9876a3790)
          (KL Divergenceí•­ì„ ì´í•­í•˜ë©´, Reconstruction Loss, KL Divergence Lossì˜ í•­ìœ¼ë¡œ êµ¬ì„±ëœ VAEì˜ Loss Function ë³¼ ìˆ˜ ìˆìŒ)
    - ë³€ë¶„ í•˜í•œì„ ìµœëŒ€í™”í•˜ë©´, ê·¼ì‚¬ ì‚¬í›„ ë¶„í¬ q(z|x)ê°€ ì‹¤ì œ ì‚¬í›„ ë¶„í¬ p(z|x)ì— ê°€ê¹Œì›Œì§
<br>

### Solution approach
1) ë³€ë¶„ í•˜í•œì„ Reparameterizationí•˜ì—¬, í‘œì¤€ í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•ìœ¼ë¡œ ìµœì í™”í•  ìˆ˜ ìˆëŠ” í•˜í•œ ì¶”ì •ì¹˜ë¥¼ ë„ì¶œ
   - SGVB(Stochastic Gradient Variational Bayes) ì¶”ì •ì¹˜ : ë³€ë¶„ í•˜í•œì„ Reparameterizationí•˜ì—¬ ë³€ë¶„ í•˜í•œì˜ ë¯¸ë¶„ ê°€ëŠ¥í•˜ê³ , í¸í–¥ë˜ì§€ ì•Šì€ ì¶”ì •ì¹˜
   - SGVBëŠ” ì—°ì†ì ì¸ ì ì¬ ë³€ìˆ˜ë¥¼ ê°€ì§„ ê±°ì˜ ëª¨ë“  ëª¨ë¸ì—ì„œ íš¨ìœ¨ì  ê·¼ì‚¬ ì‚¬í›„ ì¶”ë¡ ì— ì‚¬ìš© ê°€ëŠ¥ (í‘œì¤€ í™•ë¥ ì  ê²½ì‚¬ ìƒìŠ¹ ê¸°ë²•ì„ ì‚¬ìš©í•´ ìµœì í™” ê°€ëŠ¥)
     - í‘œì¤€ í™•ë¥ ì  ê²½ì‚¬ ìƒìŠ¹ ê¸°ë²• : í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(SGD)ë¥¼ ë³€ë¶„ ì¶”ë¡ ì˜ ELBO ìµœì í™”ì— ì ìš©í•œ ê²ƒ
     - ELBO : ìµœì í™”í•˜ë ¤ëŠ” ëª©í‘œ (ELBOë¥¼ ìµœëŒ€í™”í•˜ì—¬ q(z|x)ê°€ p(z|x)ì™€ ê°€ê¹Œì›Œì§€ë„ë¡) 
     ![image](https://github.com/user-attachments/assets/13e73c30-11c0-409c-8ff5-99f0aca6b70e)
     - SGD ì‚¬ìš© : ëª¨ë“  ë°ì´í„°ì— ëŒ€í•´ ELBO ê¸°ëŒ“ê°’ ê³„ì‚°í•˜ëŠ” ê²ƒì€ ë¹„íš¨ìœ¨ì . ì†Œê·œëª¨ mini batchë¥¼ ì‚¬ìš©í•´ ê¸°ëŒ“ê°’ì„ ê·¼ì‚¬
       
2) i.i.d.ë¥¼ ë”°ë¥´ëŠ” ë°ì´í„° ì…‹ì—ì„œ, ë°ì´í„° í¬ì¸íŠ¸ ë³„ë¡œ ì—°ì†ì ì¸ ì ì¬ ë³€ìˆ˜ê°€ ìˆëŠ” ê²½ìš°, ì œì•ˆëœ í•˜í•œ ì¶”ì •ì¹˜ë¥¼ ì‚¬ìš©í•´ 'ê³„ì‚° ë¶ˆê°€ëŠ¥í•œ ì‚¬í›„ ë¶„í¬'ë¥¼ ê·¼ì‚¬í•˜ê¸° ìœ„í•œ Recognition Model(q(z|x))ì„ í•™ìŠµì‹œí‚´ìœ¼ë¡œì¨ ì‚¬í›„ ì¶”ë¡ ì„ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰
   - Auto Encoding VB(AEVB) ì•Œê³ ë¦¬ì¦˜ ì œì•ˆ (ê° ë°ì´í„° í¬ì¸íŠ¸ë§ˆë‹¤ ë°˜ë³µì ì¸ ì¶”ë¡  (ex, MCMC)ì„ ìˆ˜í–‰í•˜ì§€ ì•Šê³ ë„ ëª¨ë¸ ë§¤ê°œ ë³€ìˆ˜ íš¨ìœ¨ì  í•™ìŠµ ê°€ëŠ¥
   - ê³¼ì • : íŒŒë¼ë¯¸í„° ì´ˆê¸°í™” -> ë°˜ë³µ(ë°ì´í„° í¬ì¸íŠ¸ ì„ íƒ -> ğœ€ ë¶„í¬ë¡œë¶€í„° random sample ì„ íƒ -> Gradients of mini batch estimator -> íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸(SGD, Adagrad ì‚¬ìš©))

<br>

### Conclusion
- SGVB ì¶”ì •ì¹˜ëŠ” í‘œì¤€ í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•ì„ ì´ìš©í•´ ë¯¸ë¶„ ë° ìµœì í™” ê°€ëŠ¥í•˜ë‹¤.
- i.i.d. ë°ì´í„°ì…‹ê³¼ ë°ì´í„° í¬ì¸íŠ¸ë‹¹ ì—°ì†ì ì¸ ì ì¬ ë³€ìˆ˜ë¥¼ ê°€ì§€ëŠ” ê²½ìš°, SGVB ì¶”ì •ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê·¼ì‚¬ ì¶”ë¡  ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” íš¨ìœ¨ì ì¸ ì¶”ë¡  ë° í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì¸ **Auto-Encoding VB(AEVB)** ë¥¼ ì†Œê°œ
   	  
<br>

### Strong points


<br>

### Weak points


<br>

### Questions


### New ideas / Comments
